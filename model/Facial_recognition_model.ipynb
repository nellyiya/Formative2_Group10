{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d3edec",
   "metadata": {},
   "source": [
    "# 1.Facial_recognition_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba778b40",
   "metadata": {},
   "source": [
    "This script builds a facial recognition model using pre-extracted facial features from images.\n",
    "It trains and compares three machine learning classifiers: Random Forest (with hyperparameter tuning),\n",
    "Logistic Regression, and XGBoost. The script includes data cleaning, feature selection, and model evaluation\n",
    "using accuracy, F1 score, and log loss. Results help identify which model performs best at recognizing individuals\n",
    "based on their facial feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7bed7",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f57ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, make_scorer\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf818b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Facial Recognition Model with Hyperparameter Tuning ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Facial Recognition Model with Hyperparameter Tuning ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc73f4",
   "metadata": {},
   "source": [
    "Step 1: Load and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94aec035",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('../image_features.csv')\n",
    "\n",
    "    # Handle infinities and missing values\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "except FileNotFoundError:\n",
    "    print(\" Error: '.../image_features.csv' not found.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\" Error loading 'image_features.csv': {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca05e1",
   "metadata": {},
   "source": [
    "Feature and target selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70beb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in df.columns if col.startswith('bin_')]\n",
    "if not feature_cols:\n",
    "    print(\" Error: No 'bin_' feature columns found.\")\n",
    "    exit()\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['member']\n",
    "\n",
    "if y.nunique() < 2:\n",
    "    print(\" Error: At least two unique member labels are required.\")\n",
    "    exit()\n",
    "\n",
    "if y.value_counts().min() < 2:\n",
    "    print(\" Warning: Some classes have very few samples. Results may be unreliable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6909ab",
   "metadata": {},
   "source": [
    "Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b188f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data Split Complete — Training: 33, Test: 15\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\" Data Split Complete — Training: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b664bf",
   "metadata": {},
   "source": [
    "Random Forest with Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74384677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Random Forest with GridSearchCV...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Djafari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Training Random Forest with GridSearchCV...\")\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "scorer = make_scorer(f1_score, average='weighted', zero_division=0)\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring=scorer,\n",
    "    cv=min(5, len(X_train) // y_train.nunique()),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "best_rf = grid_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9361bd",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42d28454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random Forest Results:\n",
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 50}\n",
      "Test Accuracy: 0.8667\n",
      "Test F1 Score: 0.8578\n",
      "Test Log Loss: 0.2693\n"
     ]
    }
   ],
   "source": [
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_proba_rf = best_rf.predict_proba(X_test)\n",
    "\n",
    "print(\" Random Forest Results:\")\n",
    "print(\"Best Parameters:\", grid_rf.best_params_)\n",
    "print(\"Test Accuracy:\", round(accuracy_score(y_test, y_pred_rf), 4))\n",
    "print(\"Test F1 Score:\", round(f1_score(y_test, y_pred_rf, average='weighted'), 4))\n",
    "print(\"Test Log Loss:\", round(log_loss(y_test, y_proba_rf), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80a25b",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0758596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Logistic Regression...\n",
      " Logistic Regression Results:\n",
      "Test Accuracy: 0.8\n",
      "Test F1 Score: 0.7673\n",
      "Test Log Loss: 1.7774\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Training Logistic Regression...\")\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "y_proba_lr = lr.predict_proba(X_test)\n",
    "\n",
    "print(\" Logistic Regression Results:\")\n",
    "print(\"Test Accuracy:\", round(accuracy_score(y_test, y_pred_lr), 4))\n",
    "print(\"Test F1 Score:\", round(f1_score(y_test, y_pred_lr, average='weighted'), 4))\n",
    "print(\"Test Log Loss:\", round(log_loss(y_test, y_proba_lr), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd8d7f",
   "metadata": {},
   "source": [
    "XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7009f98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Djafari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:52:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training XGBoost Classifier...\n",
      " XGBoost Results:\n",
      "Test Accuracy: 0.7333\n",
      "Test F1 Score: 0.6978\n",
      "Test Log Loss: 0.6344\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Training XGBoost Classifier...\")\n",
    "\n",
    "# Encode labels for XGBoost\n",
    "label_map = {label: idx for idx, label in enumerate(y.unique())}\n",
    "y_train_enc = y_train.map(label_map)\n",
    "y_test_enc = y_test.map(label_map)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train_enc)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test)\n",
    "\n",
    "print(\" XGBoost Results:\")\n",
    "print(\"Test Accuracy:\", round(accuracy_score(y_test_enc, y_pred_xgb), 4))\n",
    "print(\"Test F1 Score:\", round(f1_score(y_test_enc, y_pred_xgb, average='weighted'), 4))\n",
    "print(\"Test Log Loss:\", round(log_loss(y_test_enc, y_proba_xgb), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
